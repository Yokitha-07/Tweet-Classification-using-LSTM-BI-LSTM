
# ğŸ§  Tweet Classification using LSTM & Bi-LSTM

This project focuses on classifying tweets as **personal health mentions** or **non-personal health mentions** using deep learning techniques.  
The main goal is to build an **LSTM-based text classification model** that understands the context of each tweet to make accurate predictions.

---

## ğŸ“‹ Project Overview

Social media platforms like Twitter contain massive amounts of text data related to health discussions.  
This project aims to automatically identify whether a tweet refers to a **personal health condition**, which can support **public health monitoring and analysis**.

The project involves:
- Text preprocessing and cleaning
- Tokenization and padding
- Building and training LSTM and Bi-LSTM models
- Evaluating performance on classification metrics

---

## ğŸš€ Key Features

- Implemented **LSTM** and **Bidirectional LSTM** models for sequential text analysis  
- Applied **Word Embeddings (Word2Vec/Embedding Layer)** for representing words in vector space  
- Preprocessed data using **tokenization**, **stopword removal**, and **padding sequences**  
- Achieved around **77â€“79% accuracy** on validation data  
- Built using **TensorFlow / Keras** and **Python**

---

## ğŸ§° Tech Stack

| Category | Technologies |
|-----------|---------------|
| Language | Python |
| Frameworks | TensorFlow, Keras |
| Libraries | NumPy, Pandas, Matplotlib, Scikit-learn |
| Environment | Google Colab / Jupyter Notebook |

---

## ğŸ§© Model Architecture

1. **Embedding Layer** â€“ Converts words into dense vector representations  
2. **LSTM Layer** â€“ Captures sequential dependencies in tweets  
3. **Dropout Layer** â€“ Prevents overfitting  
4. **Dense Output Layer** â€“ Sigmoid activation for binary classification  

You can also experiment with **Bidirectional LSTM** to capture both forward and backward context.

---

## ğŸ§¼ Data Preprocessing Steps

- Lowercasing text  
- Removing punctuation, numbers, and special characters  
- Removing stopwords  
- Tokenization and sequence padding  

---

## ğŸ“Š Results

| Model | Accuracy | Loss |
|--------|-----------|------|
| LSTM | ~77% | - |
| Bi-LSTM | ~79% | - |

ğŸ§‘â€ğŸ’» Author

Yokitha R
ğŸ“ University of Peradeniya, Sri Lanka
ğŸ’¼ Aspiring AI & ML Intern
]

---


